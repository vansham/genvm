use crate::{common::ModuleResult, scripting};
use anyhow::Context;
use base64::Engine;
use genvm_common::*;

use super::{config, prompt};

#[async_trait::async_trait]
pub trait Provider {
    async fn exec_prompt_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String>;

    async fn exec_prompt_json_as_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        self.exec_prompt_text(ctx, prompt, model).await
    }

    async fn exec_prompt_json(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<serde_json::Map<String, serde_json::Value>> {
        let res = self.exec_prompt_json_as_text(ctx, prompt, model).await?;
        let res = sanitize_json_str(&res);
        let res = serde_json::from_str(&res).with_context(|| format!("parsing {res:?}"))?;

        Ok(res)
    }

    async fn exec_prompt_bool_reason(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<bool> {
        let result = self.exec_prompt_json(ctx, prompt, model).await?;
        let res = result.get("result").and_then(|x| x.as_bool());

        if let Some(res) = res {
            Ok(res)
        } else {
            log_error!(result:? = result; "no result in reason, returning false");

            Ok(false)
        }
    }
}

pub struct OpenAICompatible {
    pub(crate) config: config::BackendConfig,
}

pub struct Gemini {
    pub(crate) config: config::BackendConfig,
}

pub struct OLlama {
    pub(crate) config: config::BackendConfig,
}

pub struct Anthropic {
    pub(crate) config: config::BackendConfig,
}

impl prompt::Internal {
    fn to_openai_messages(&self) -> ModuleResult<Vec<serde_json::Value>> {
        let mut messages = Vec::new();
        if let Some(sys) = &self.system_message {
            messages.push(serde_json::json!({
                "role": "system",
                "content": sys,
            }));
        }

        let mut user_content = Vec::new();

        user_content.push(serde_json::json!({
            "type": "text",
            "text": self.user_message,
        }));

        for img in &self.images {
            let mut encoded = "data:".to_owned();
            let kind = img.kind_or_error()?;
            encoded.push_str(kind.media_type());
            encoded.push_str(";base64,");
            base64::prelude::BASE64_STANDARD.encode_string(&img.0, &mut encoded);

            user_content.push(serde_json::json!({
                "type": "image_url",
                "image_url": { "url": encoded },
            }));
        }

        messages.push(serde_json::json!({
            "role": "user",
            "content": user_content,
        }));

        Ok(messages)
    }

    fn add_gemini_messages(
        &self,
        to: &mut serde_json::Map<String, serde_json::Value>,
    ) -> ModuleResult<()> {
        if let Some(sys) = &self.system_message {
            to.insert(
                "system_instruction".to_owned(),
                serde_json::json!({
                    "parts": [{"text": sys}],
                }),
            );
        }

        let mut parts = Vec::new();
        for img in &self.images {
            let kind = img.kind_or_error()?;
            parts.push(serde_json::json!({
                "inline_data": {
                    "mime_type": kind.media_type(),
                    "data": img.as_base64(),
                }
            }));
        }
        parts.push(serde_json::json!({"text": self.user_message}));

        to.insert(
            "contents".to_owned(),
            serde_json::json!([{
                "parts": parts,
            }]),
        );

        Ok(())
    }
}

#[async_trait::async_trait]
impl Provider for OpenAICompatible {
    async fn exec_prompt_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let mut request = serde_json::json!({
            "model": model,
            "messages": prompt.to_openai_messages()?,
            "stream": false,
            "temperature": prompt.temperature,
        });

        if prompt.use_max_completion_tokens {
            request
                .as_object_mut()
                .unwrap()
                .insert("max_completion_tokens".to_owned(), prompt.max_tokens.into());
        } else {
            request
                .as_object_mut()
                .unwrap()
                .insert("max_tokens".to_owned(), prompt.max_tokens.into());
        }

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/v1/chat/completions", self.config.host);
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Authorization", &format!("Bearer {}", &self.config.key))
            .body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let response = res
            .body
            .pointer("/choices/0/message/content")
            .and_then(|v| v.as_str())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;

        Ok(response.to_owned())
    }

    async fn exec_prompt_json(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<serde_json::Map<String, serde_json::Value>> {
        let mut request = serde_json::json!({
            "model": model,
            "messages": prompt.to_openai_messages()?,
            "stream": false,
            "temperature": prompt.temperature,
            "response_format": {"type": "json_object"},
        });

        if prompt.use_max_completion_tokens {
            request
                .as_object_mut()
                .unwrap()
                .insert("max_completion_tokens".to_owned(), prompt.max_tokens.into());
        } else {
            request
                .as_object_mut()
                .unwrap()
                .insert("max_tokens".to_owned(), prompt.max_tokens.into());
        }

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/v1/chat/completions", self.config.host);
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("Authorization", &format!("Bearer {}", &self.config.key))
            .body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let response = res
            .body
            .pointer("/choices/0/message/content")
            .and_then(|v| v.as_str())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;

        let response = sanitize_json_str(response);
        let response =
            serde_json::from_str(&response).with_context(|| format!("parsing {response:?}"))?;

        Ok(response)
    }
}

impl prompt::Internal {
    fn to_ollama_no_format(&self, model: &str) -> serde_json::Value {
        let mut request = serde_json::json!({
            "model": model,
            "prompt": self.user_message,
            "stream": false,
            "options": {
                "temperature": self.temperature,
                "num_predict": self.max_tokens,
            },
        });

        let mut images = Vec::new();
        for img in &self.images {
            images.push(serde_json::Value::String(img.as_base64()));
        }
        request
            .as_object_mut()
            .unwrap()
            .insert("images".into(), serde_json::Value::Array(images));

        if let Some(sys) = &self.system_message {
            request
                .as_object_mut()
                .unwrap()
                .insert("system".into(), sys.to_owned().into());
        }

        request
    }
}

#[async_trait::async_trait]
impl Provider for OLlama {
    async fn exec_prompt_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let request = prompt.to_ollama_no_format(model);

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/api/generate", self.config.host);
        let request = ctx.client.post(&url).body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let response = res
            .body
            .as_object()
            .and_then(|v| v.get("response"))
            .and_then(|v| v.as_str())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;
        Ok(response.to_owned())
    }

    async fn exec_prompt_json_as_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let mut request = prompt.to_ollama_no_format(model);

        request
            .as_object_mut()
            .unwrap()
            .insert("format".into(), "json".into());

        let mut images = Vec::new();
        for img in &prompt.images {
            images.push(serde_json::Value::String(img.as_base64()));
        }

        if !images.is_empty() {
            request
                .as_object_mut()
                .unwrap()
                .insert("images".into(), serde_json::Value::Array(images));
        }

        if let Some(sys) = &prompt.system_message {
            request
                .as_object_mut()
                .unwrap()
                .insert("system".into(), sys.to_owned().into());
        }

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/api/generate", self.config.host);
        let request = ctx.client.post(&url).body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let response = res
            .body
            .as_object()
            .and_then(|v| v.get("response"))
            .and_then(|v| v.as_str())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;
        Ok(response.to_owned())
    }
}

#[async_trait::async_trait]
impl Provider for Gemini {
    async fn exec_prompt_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let mut request = serde_json::json!({
            "generationConfig": {
                "responseMimeType": "text/plain",
                "temperature": prompt.temperature,
                "maxOutputTokens": prompt.max_tokens,
            }
        });

        prompt.add_gemini_messages(request.as_object_mut().unwrap())?;

        let request = serde_json::to_vec(&request)?;
        let url = format!(
            "{}/v1beta/models/{}:generateContent?key={}",
            self.config.host, model, self.config.key
        );
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .body(request.clone());
        let res_json = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let res = res_json
            .body
            .pointer("/candidates/0/content/parts/0/text")
            .and_then(|x| x.as_str());

        if res.is_none()
            && res_json
                .body
                .pointer("/candidates/0/finishReason")
                .and_then(|x| x.as_str())
                == Some("MAX_TOKENS")
        {
            return Ok("".into());
        }

        let res =
            res.ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res_json.body))?;
        Ok(res.into())
    }

    async fn exec_prompt_json_as_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let mut request = serde_json::json!({
            "generationConfig": {
                "responseMimeType": "application/json",
                "temperature": prompt.temperature,
                "maxOutputTokens": prompt.max_tokens,
            }
        });

        prompt.add_gemini_messages(request.as_object_mut().unwrap())?;

        let request = serde_json::to_vec(&request)?;
        let url = format!(
            "{}/v1beta/models/{}:generateContent?key={}",
            self.config.host, model, self.config.key
        );
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .body(request.clone());
        let res_json = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let res = res_json
            .body
            .pointer("/candidates/0/content/parts/0/text")
            .and_then(|x| x.as_str());

        if !res.map(|x| x.starts_with("{")).unwrap_or(false)
            && res_json
                .body
                .pointer("/candidates/0/finishReason")
                .and_then(|x| x.as_str())
                == Some("MAX_TOKENS")
        {
            return Ok("{}".to_owned());
        }

        let res =
            res.ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res_json.body))?;

        Ok(res.to_owned())
    }
}

impl prompt::Internal {
    fn to_anthropic_no_format(&self, model: &str) -> ModuleResult<serde_json::Value> {
        let mut user_content = Vec::new();

        for img in &self.images {
            let kind = img.kind_or_error()?;
            user_content.push(serde_json::json!({"type": "image", "source": {
                "type": "base64",
                "media_type": kind.media_type(),
                "data": img.as_base64(),
            }}));
        }

        user_content.push(serde_json::json!({"type": "text", "text": self.user_message}));

        let mut request = serde_json::json!({
            "model": model,
            "messages": [{"role": "user", "content": user_content}],
            "max_tokens": self.max_tokens,
            "stream": false,
            "temperature": self.temperature,
        });

        if let Some(sys) = &self.system_message {
            request
                .as_object_mut()
                .unwrap()
                .insert("system".into(), sys.to_owned().into());
        }

        Ok(request)
    }
}

#[async_trait::async_trait]
impl Provider for Anthropic {
    async fn exec_prompt_text(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<String> {
        let request = prompt.to_anthropic_no_format(model)?;

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/v1/messages", self.config.host);
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("x-api-key", &self.config.key)
            .header("anthropic-version", "2023-06-01")
            .body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        res.body
            .pointer("/content/0/text")
            .and_then(|x| x.as_str())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))
            .map(String::from)
    }

    async fn exec_prompt_json(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<serde_json::Map<String, serde_json::Value>> {
        let mut request = prompt.to_anthropic_no_format(model)?;

        request.as_object_mut().unwrap().insert(
            "tools".to_owned(),
            serde_json::json!(
                [{
                    "name": "json_out",
                    "description": "Output a valid json object",
                    "input_schema": {
                        "type": "object",
                        "patternProperties": {
                            "": {
                                "type": ["object", "null", "array", "number", "string"],
                            }
                        },
                    }
                }]
            ),
        );
        request.as_object_mut().unwrap().insert(
            "tool_choice".to_owned(),
            serde_json::json!({
                "type": "tool",
                "name": "json_out"
            }),
        );

        if let Some(sys) = &prompt.system_message {
            request
                .as_object_mut()
                .unwrap()
                .insert("system".into(), sys.to_owned().into());
        }

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/v1/messages", self.config.host);
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("x-api-key", &self.config.key)
            .header("anthropic-version", "2023-06-01")
            .body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let val = res
            .body
            .pointer("/content/0/input")
            .and_then(|x| x.as_object())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;

        Ok(val.clone())
    }

    async fn exec_prompt_bool_reason(
        &self,
        ctx: &scripting::CtxPart,
        prompt: &prompt::Internal,
        model: &str,
    ) -> ModuleResult<bool> {
        let mut request = serde_json::json!({
            "model": model,
            "messages": [{"role": "user", "content": prompt.user_message}],
            "max_tokens": 200,
            "stream": false,
            "temperature": prompt.temperature,
            "tools": [{
                "name": "json_out",
                "description": "Output a valid json object",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "result": { "type": "boolean" },
                        "reason": { "type": "string" },
                    },
                    "required": ["result"],
                }
            }],
            "tool_choice": {
                "type": "tool",
                "name": "json_out"
            }
        });

        if let Some(sys) = &prompt.system_message {
            request
                .as_object_mut()
                .unwrap()
                .insert("system".into(), sys.to_owned().into());
        }

        let request = serde_json::to_vec(&request)?;
        let url = format!("{}/v1/messages", self.config.host);
        let request = ctx
            .client
            .post(&url)
            .header("Content-Type", "application/json")
            .header("x-api-key", &self.config.key)
            .header("anthropic-version", "2023-06-01")
            .body(request.clone());
        let res = scripting::send_request_get_lua_compatible_response_json(
            &ctx.metrics,
            &url,
            request,
            true,
        )
        .await?;

        let val = res
            .body
            .pointer("/content/0/input/result")
            .and_then(|x| x.as_bool())
            .ok_or_else(|| anyhow::anyhow!("can't get response field {}", &res.body))?;

        Ok(val)
    }
}

fn sanitize_json_str(s: &str) -> String {
    let s = s.trim();
    let s = s
        .strip_prefix("```json")
        .or(s.strip_prefix("```"))
        .unwrap_or(s);
    let s = s.strip_suffix("```").unwrap_or(s);
    let s = s.trim();

    genvm_modules::complete_json(s)
}

#[cfg(test)]
#[allow(non_upper_case_globals, dead_code)]
mod tests {
    use std::collections::BTreeMap;
    use std::collections::HashMap;

    use crate::common;
    use crate::scripting;

    use super::super::{config, prompt};
    use genvm_common::sync;
    use genvm_common::templater;

    fn is_overloaded(e: &anyhow::Error) -> bool {
        let e = match e.downcast_ref::<common::ModuleError>() {
            None => return false,
            Some(e) => e,
        };

        if !e
            .causes
            .iter()
            .any(|e| e == &common::ErrorKind::STATUS_NOT_OK.to_string())
        {
            return true;
        }

        match e.ctx.get("status").and_then(|x| x.as_num()) {
            None => false,
            Some(status) => [408, 503, 429, 504, 529].contains(&(status as i32)),
        }
    }

    mod conf {
        pub const openai: &str = r#"{
            "host": "https://api.openai.com",
            "provider": "openai-compatible",
            "models": {
                "gpt-4o-mini": { "supports_json": true }
            },
            "key": "${ENV[OPENAIKEY]}"
        }"#;

        pub const heurist: &str = r#"{
            "host": "https://llm-gateway.heurist.xyz",
            "provider": "openai-compatible",
            "models": {
                "meta-llama/llama-3.3-70b-instruct": { "supports_json": true }
            },
            "key": "${ENV[HEURISTKEY]}"
        }"#;

        pub const heurist_deepseek: &str = r#"{
            "host": "https://llm-gateway.heurist.xyz",
            "provider": "openai-compatible",
            "models": {
                "deepseek/deepseek-v3": { "supports_json": true }
            },
            "key": "${ENV[HEURISTKEY]}"
        }"#;

        pub const anthropic: &str = r#"{
            "host": "https://api.anthropic.com",
            "provider": "anthropic",
            "models": { "claude-haiku-4-5-20251001" : {} },
            "key": "${ENV[ANTHROPICKEY]}"
        }"#;

        pub const xai: &str = r#"{
            "host": "https://api.x.ai",
            "provider": "openai-compatible",
            "models": { "grok-3" : { "supports_json": true } },
            "key": "${ENV[XAIKEY]}"
        }"#;

        pub const google: &str = r#"{
            "host": "https://generativelanguage.googleapis.com",
            "provider": "google",
            "models": { "gemini-2.5-flash": { "supports_json": true } },
            "key": "${ENV[GEMINIKEY]}"
        }"#;

        pub const atoma: &str = r#"{
            "host": "https://api.atoma.network",
            "provider": "openai-compatible",
            "models": { "meta-llama/Llama-3.3-70B-Instruct": {} },
            "key": "${ENV[ATOMAKEY]}"
        }"#;
    }

    async fn do_test_text(conf: &str) {
        common::tests::setup();

        let backend: serde_json::Value = serde_json::from_str(conf).unwrap();
        let mut vars = HashMap::new();
        for (mut name, value) in std::env::vars() {
            name.insert_str(0, "ENV[");
            name.push(']');

            vars.insert(name, value);
        }
        let backend =
            genvm_common::templater::patch_json(&vars, backend, &templater::DOLLAR_UNFOLDER_RE)
                .unwrap();
        let backend: config::BackendConfig = serde_json::from_value(backend).unwrap();
        let provider = backend.to_provider();

        let ctx = scripting::CtxPart {
            client: common::create_client().unwrap(),
            metrics: sync::DArc::new(scripting::Metrics::default()),
            node_address: "test_node".to_owned(),
            sign_headers: std::sync::Arc::new(BTreeMap::new()),
            sign_url: std::sync::Arc::from("test_url"),
            sign_vars: BTreeMap::new(),
            hello: std::sync::Arc::new(genvm_modules_interfaces::GenVMHello {
                genvm_id: genvm_modules_interfaces::GenVMId(999),
                host_data: genvm_modules_interfaces::HostData {
                    tx_id: "test_tx".to_owned(),
                    node_address: "test_node".to_owned(),
                    rest: serde_json::Map::new(),
                },
            }),
        };

        let res = provider
            .exec_prompt_text(
                &ctx,
                &prompt::Internal {
                    system_message: None,
                    temperature: 0.7,
                    user_message: "Respond with a single word \"yes\" (without quotes) and only this word, lowercase".to_owned(),
                    images: Vec::new(),
                    max_tokens: 500,
                    use_max_completion_tokens: true,
                },
                backend.script_config.models.first_key_value().unwrap().0,
            )
            .await;

        let res = match res {
            Ok(res) => res,
            Err(e) if is_overloaded(&e) => {
                eprintln!("Overloaded, skipping test: {e}");
                return;
            }
            Err(e) => {
                panic!("test failed: {e}");
            }
        };

        let res = res.trim().to_lowercase();

        assert_eq!(res, "yes");
    }

    const BIG_PROMPT: &str = r#"
        ðŸŒ† Poem Prompt: The Shadow Citizen
        Task: Write a poem, approximately 16-20 lines, about the urban rat. Your goal is to move beyond the simple idea of "pest" and explore the rat as a complex, parallel inhabitant of the city.

        Core Theme: Focus on the rat as a secret-keeper or a historian of the discarded. It moves through the spaces we ignoreâ€”the subway tunnels, the forgotten foundations, the labyrinth of pipes. It thrives on what we throw away.

        Guiding Questions & Imagery:

        Perspective: Is the poem from the rat's point of view, or from an observer who suddenly sees the rat in a new light?

        Sensory Details: What does it hear? The "rumble of the steel train" from below? The "whispers of the lost" in the alley?

        The "Kingdom": Describe its environment. Is it a "concrete maze," a "kingdom of rust and refuse," or a "shadow empire"?

        Contrast: How does its quick, intelligent, and cautious life contrast with the loud, oblivious human world above? Consider its "onyx eye" reflecting the "neon glare."

        Your challenge: Craft a portrait that is both gritty and graceful. Acknowledge its maligned status but give it a sense of agency, intelligence, and undeniable belonging to the city's hidden pulse.
    "#;

    async fn do_test_text_out_of_tokens(conf: &str) {
        common::tests::setup();

        let backend: serde_json::Value = serde_json::from_str(conf).unwrap();
        let mut vars = HashMap::new();
        for (mut name, value) in std::env::vars() {
            name.insert_str(0, "ENV[");
            name.push(']');

            vars.insert(name, value);
        }
        let backend =
            genvm_common::templater::patch_json(&vars, backend, &templater::DOLLAR_UNFOLDER_RE)
                .unwrap();
        let backend: config::BackendConfig = serde_json::from_value(backend).unwrap();
        let provider = backend.to_provider();

        let ctx = scripting::CtxPart {
            client: common::create_client().unwrap(),
            metrics: sync::DArc::new(scripting::Metrics::default()),
            node_address: "test_node".to_owned(),
            sign_headers: std::sync::Arc::new(BTreeMap::new()),
            sign_url: std::sync::Arc::from("test_url"),
            sign_vars: BTreeMap::new(),
            hello: std::sync::Arc::new(genvm_modules_interfaces::GenVMHello {
                genvm_id: genvm_modules_interfaces::GenVMId(999),
                host_data: genvm_modules_interfaces::HostData {
                    tx_id: "test_tx".to_owned(),
                    node_address: "test_node".to_owned(),
                    rest: serde_json::Map::new(),
                },
            }),
        };

        let res = provider
            .exec_prompt_text(
                &ctx,
                &prompt::Internal {
                    system_message: None,
                    temperature: 0.7,
                    user_message: BIG_PROMPT.to_owned(),
                    images: Vec::new(),
                    max_tokens: 50,
                    use_max_completion_tokens: true,
                },
                backend.script_config.models.first_key_value().unwrap().0,
            )
            .await;

        let res = match res {
            Ok(res) => res,
            Err(e) if is_overloaded(&e) => {
                eprintln!("Overloaded, skipping test: {e}");
                return;
            }
            Err(e) => {
                panic!("test failed: {e}");
            }
        };

        let res = res.trim().to_lowercase();

        println!("result is {res}");
    }

    async fn do_test_json(conf: &str) {
        common::tests::setup();

        let backend: serde_json::Value = serde_json::from_str(conf).unwrap();
        let mut vars = HashMap::new();
        for (mut name, value) in std::env::vars() {
            name.insert_str(0, "ENV[");
            name.push(']');

            vars.insert(name, value);
        }
        let backend =
            genvm_common::templater::patch_json(&vars, backend, &templater::DOLLAR_UNFOLDER_RE)
                .unwrap();
        let backend: config::BackendConfig = serde_json::from_value(backend).unwrap();

        if !backend
            .script_config
            .models
            .first_key_value()
            .unwrap()
            .1
            .supports_json
        {
            return;
        }

        let provider = backend.to_provider();

        let ctx = scripting::CtxPart {
            client: common::create_client().unwrap(),
            metrics: sync::DArc::new(scripting::Metrics::default()),
            node_address: "test_node".to_owned(),
            sign_headers: std::sync::Arc::new(BTreeMap::new()),
            sign_url: std::sync::Arc::from("test_url"),
            sign_vars: BTreeMap::new(),
            hello: std::sync::Arc::new(genvm_modules_interfaces::GenVMHello {
                genvm_id: genvm_modules_interfaces::GenVMId(999),
                host_data: genvm_modules_interfaces::HostData {
                    tx_id: "test_tx".to_owned(),
                    node_address: "test_node".to_owned(),
                    rest: serde_json::Map::new(),
                },
            }),
        };

        const PROMPT: &str = r#"respond with json object containing single key "result" and associated value being a random integer from 0 to 100 (inclusive), it must be number, not wrapped in quotes. This object must not be wrapped into other objects. Example: {"result": 10}"#;
        let res = provider
            .exec_prompt_json(
                &ctx,
                &prompt::Internal {
                    system_message: Some("respond with json".to_owned()),
                    temperature: 0.7,
                    user_message: PROMPT.to_owned(),
                    images: Vec::new(),
                    max_tokens: 500,
                    use_max_completion_tokens: true,
                },
                backend.script_config.models.first_key_value().unwrap().0,
            )
            .await;
        eprintln!("{res:?}");

        let res = match res {
            Ok(res) => res,
            Err(e) if is_overloaded(&e) => {
                eprintln!("Overloaded, skipping test: {e}");
                return;
            }
            Err(e) => {
                panic!("test failed: {e}");
            }
        };

        let as_val = serde_json::Value::Object(res);

        // all this because of anthropic
        for potential in [
            as_val.pointer("/result").and_then(|x| x.as_i64()),
            as_val.pointer("/root/result").and_then(|x| x.as_i64()),
            as_val.pointer("/json/result").and_then(|x| x.as_i64()),
            as_val.pointer("/type/result").and_then(|x| x.as_i64()),
            as_val.pointer("/object/result").and_then(|x| x.as_i64()),
            as_val.pointer("/value/result").and_then(|x| x.as_i64()),
            as_val.pointer("/data/result").and_then(|x| x.as_i64()),
            as_val.pointer("/response/result").and_then(|x| x.as_i64()),
            as_val.pointer("/answer/result").and_then(|x| x.as_i64()),
        ] {
            if let Some(v) = potential {
                assert!((0..=100).contains(&v));
                return;
            }
        }
        unreachable!("no result found in {as_val:?}");
    }

    async fn do_test_json_out_of_tokens(conf: &str) {
        common::tests::setup();

        let backend: serde_json::Value = serde_json::from_str(conf).unwrap();
        let mut vars = HashMap::new();
        for (mut name, value) in std::env::vars() {
            name.insert_str(0, "ENV[");
            name.push(']');

            vars.insert(name, value);
        }
        let backend =
            genvm_common::templater::patch_json(&vars, backend, &templater::DOLLAR_UNFOLDER_RE)
                .unwrap();
        let backend: config::BackendConfig = serde_json::from_value(backend).unwrap();

        if !backend
            .script_config
            .models
            .first_key_value()
            .unwrap()
            .1
            .supports_json
        {
            return;
        }

        let provider = backend.to_provider();

        let ctx = scripting::CtxPart {
            client: common::create_client().unwrap(),
            metrics: sync::DArc::new(scripting::Metrics::default()),
            node_address: "test_node".to_owned(),
            sign_headers: std::sync::Arc::new(BTreeMap::new()),
            sign_url: std::sync::Arc::from("test_url"),
            sign_vars: BTreeMap::new(),
            hello: std::sync::Arc::new(genvm_modules_interfaces::GenVMHello {
                genvm_id: genvm_modules_interfaces::GenVMId(999),
                host_data: genvm_modules_interfaces::HostData {
                    tx_id: "test_tx".to_owned(),
                    node_address: "test_node".to_owned(),
                    rest: serde_json::Map::new(),
                },
            }),
        };

        const PROMPT: &str = r#"respond with json object containing two keys. First key is a poem about rats and second key "result" and associated value being a random integer from 0 to 100 (inclusive), it must be number, not wrapped in quotes. This object must not be wrapped into other objects. Example: {"poem": "A kingdom built of rust and steam, Beneath the concrete, cold and vast, He navigates the broken dream, A living shadow, built to last. He slips between the pipe and wire, A citizen of drain and seam, Ignoring all the surface fire Of our oblivious, waking stream.", "result": 10}"#;
        let res = provider
            .exec_prompt_json(
                &ctx,
                &prompt::Internal {
                    system_message: Some("respond with json".to_owned()),
                    temperature: 0.7,
                    user_message: PROMPT.to_owned(),
                    images: Vec::new(),
                    max_tokens: 50,
                    use_max_completion_tokens: true,
                },
                backend.script_config.models.first_key_value().unwrap().0,
            )
            .await;
        eprintln!("{res:?}");

        match res {
            Ok(res) => res,
            Err(e) if is_overloaded(&e) => {
                eprintln!("Overloaded, skipping test: {e}");
                return;
            }
            Err(e) => {
                panic!("test failed: {e}");
            }
        };
    }

    macro_rules! make_test {
        ($conf:ident) => {
            mod $conf {
                use crate::common;

                #[tokio::test]
                async fn text() {
                    let conf = super::conf::$conf;
                    common::test_with_genvm_id(genvm_modules_interfaces::GenVMId(999), async {
                        super::do_test_text(conf).await
                    })
                    .await;
                }
                #[tokio::test]
                async fn json() {
                    let conf = super::conf::$conf;
                    common::test_with_genvm_id(genvm_modules_interfaces::GenVMId(999), async {
                        super::do_test_json(conf).await
                    })
                    .await;
                }

                #[tokio::test]
                async fn text_out_of_tokens() {
                    let conf = super::conf::$conf;
                    common::test_with_genvm_id(genvm_modules_interfaces::GenVMId(999), async {
                        super::do_test_text_out_of_tokens(conf).await
                    })
                    .await;
                }

                #[tokio::test]
                async fn json_out_of_tokens() {
                    let conf = super::conf::$conf;
                    common::test_with_genvm_id(genvm_modules_interfaces::GenVMId(999), async {
                        super::do_test_json_out_of_tokens(conf).await
                    })
                    .await;
                }
            }
        };
    }

    make_test!(openai);
    make_test!(anthropic);
    make_test!(google);
    make_test!(xai);

    make_test!(heurist);
    make_test!(heurist_deepseek);
    //make_test!(atoma);
}
